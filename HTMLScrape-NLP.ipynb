{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code performs natural langauge processing on raw HTML files from the training data \n",
    "# The inputs are a random sample of some user-specified fraction of the ~337000 files in the training set.\n",
    "#\n",
    "#\n",
    "# The output is given in WordCounts.csv, saved in the working directory.\n",
    "# WordCounts.csv is then taken as an input for FeatureSelection-Words.ipynb\n",
    "#\n",
    "#\n",
    "# WordCounts.csv has the following format:\n",
    "#\n",
    "# word_1 word_2 ... word_n filename\n",
    "#   0      1           0   3093804_raw_html.txt\n",
    "#   1      0           1   845185_raw_html.txt\n",
    "#   ...   ...         ...     ...\n",
    "#\n",
    "# The boolean variable denotes whether or not that particular word appeared twice or \n",
    "# more in the visible text for a particular file. \n",
    "# The word list: word_1, word_2, ... word_n corresponds to common words, specified below in selected_words\n",
    "#\n",
    "# This cell contains function definitions and parameter initializations\n",
    "# Input and output directories are sepecified in the next cell.\n",
    "#\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import multiprocessing\n",
    "import sys\n",
    "import time\n",
    "import enchant\n",
    "import random\n",
    "from __future__ import print_function\n",
    "\n",
    "# Initialize stopwords for natural language processing (taken from NLTK list)\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n",
    "# Use U.S. dictionary from enchanct\n",
    "english_dict = enchant.Dict(\"en_US\")\n",
    "\n",
    "\n",
    "# Function which flattens a list\n",
    "def flatten(list_):\n",
    "    return [item for sublist in list_ for item in sublist]\n",
    "\n",
    "\n",
    "# Function which returns True if \"element\" in HTML file is visible \n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Function returns True if inputString satisfies selection criteria. \n",
    "#\n",
    "# If selected_words is not empty, will only return true if inputString is in selected_words\n",
    "#\n",
    "# Else if selected_words is empty, will return true if inputString:\n",
    "# (contains no numbers) && (not in stopwords) && (in english_dict or is a single character).\n",
    "#\n",
    "def Keep_String(inputString):\n",
    "    if selected_words:\n",
    "        selected = inputString in selected_words\n",
    "    else:\n",
    "        ContainsNumbers = bool(re.search(r'\\d', inputString))\n",
    "        stopword = inputString in stopwords\n",
    "        single_char = len(inputString) == 1\n",
    "        in_dictionary = english_dict.check(inputString)\n",
    "        selected = not ContainsNumbers and not stopword and (single_char or in_dictionary)\n",
    "#    \n",
    "    return selected\n",
    "\n",
    "# Function which counts words in word_list, and returns word count in dict format.\n",
    "# Adds word counts to input dictionary in_dict (can be empty), and returns resulting dictionary.\n",
    "# If unique = True, only apply word counting to unique word list.\n",
    "#\n",
    "def count_words(word_list,in_dict,unique):\n",
    "    if unique:\n",
    "        input_list = np.unique(word_list)\n",
    "    else:\n",
    "        input_list = word_list\n",
    "    for word in input_list:\n",
    "        if word in in_dict:\n",
    "            in_dict[word] += 1\n",
    "        else:\n",
    "            in_dict[word] = 1\n",
    "    return in_dict\n",
    "\n",
    "\n",
    "# Function which takes in HTML file (filename) and tokenizes visible words.\n",
    "# Output is list of resulting words, filtered with the Keep_String function\n",
    "#\n",
    "def get_text(filename):\n",
    "    in_file = open(filename,'r')\n",
    "    parser = BeautifulSoup(in_file, 'html.parser')\n",
    "    texts = parser.findAll(text=True)\n",
    "    visible_texts = filter(visible,texts)\n",
    "    text_dump = []\n",
    "# Lower and remove newlines\n",
    "    for item in visible_texts:\n",
    "        text_dump.append(str(item.encode('utf-8').lower()).strip('\\n'))\n",
    "# Remove punctiaton and split into words\n",
    "    tokenized_words = filter(None,[re.sub(\"[^\\w]\", \" \",item).split() for item in text_dump])\n",
    "    word_list =flatten(tokenized_words)\n",
    "    return filter(Keep_String,word_list)\n",
    "#\n",
    "#\n",
    "# \"Main\" function. Takes in a HTML file (path specificed in variable filepath).\n",
    "# Outputs dictionary to append to WordCounts.csv\n",
    "#\n",
    "def get_word_counts(filepath):\n",
    "    # Obtain tokenized word list\n",
    "    word_list = get_text(filepath)\n",
    "    # Obtain output dictionary where: \n",
    "    # key = word in selected word, value = 1 if word appears at least twice, else value = 0\n",
    "    output_dict = {}\n",
    "    for word in selected_words:\n",
    "        if word_list.count(word) > 1:\n",
    "            output_dict[word] = 1\n",
    "        else:\n",
    "            output_dict[word] = 0\n",
    "    # Add filename to dictionary\n",
    "    output_dict[\"file_name\"] = os.path.basename(filepath)\n",
    "    return output_dict\n",
    "#\n",
    "#\n",
    "selected_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set input directory (paths containing training data)\n",
    "#\n",
    "input_dir_train = \"/media/sf_VboxShar/Native_Advertising_train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate list of common words to keep, store in selected_words. \n",
    "#\n",
    "# Obtain list by sampling 5000 files from training data.\n",
    "#\n",
    "WordSelectSamples = 5000\n",
    "filepaths = glob.glob(input_dir_train + '*.txt')\n",
    "random.shuffle(filepaths)\n",
    "filepaths = filepaths[0:WordSelectSamples]\n",
    "#\n",
    "#\n",
    "# Aim for ~1000 of the most common words, to keep WordCounts.csv from becoming huge.\n",
    "#\n",
    "#\n",
    "# cum_dict is dictionary where: \n",
    "# key = word, value = number of files where word appeared at least twice\n",
    "#\n",
    "cum_dict = dict()\n",
    "for i, filename in enumerate(filepaths):\n",
    "#   get_text tokenizes visible words from raw the HTML file into a list, see function def. for details\n",
    "    word_list = get_text(filename)\n",
    "    cum_dict = count_words(word_list,cum_dict,unique = True)\n",
    "    print(\"\\r Reading file {:,} out of {:,}\".format(i+1,WordSelectSamples), end='')\n",
    "    sys.stdout.flush()\n",
    "#    \n",
    "# Convert cum_dict to dataframe\n",
    "cum_count = pd.DataFrame(list(cum_dict.iteritems()),columns=[\"word_\",\"word_count\"])\n",
    "#\n",
    "# keep words that appear at least twice in more than mincount documents.\n",
    "mincount=int(WordSelectSamples*0.065)\n",
    "pruned_cum_count = cum_count[ ( cum_count[\"word_count\"] > mincount)]\n",
    "# \n",
    "selected_words = pruned_cum_count[\"word_\"].tolist()\n",
    "del cum_count\n",
    "del cum_dict\n",
    "del pruned_cum_count\n",
    "print(\"  Number of selected words: {}\".format(len(selected_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "# Set fraction of training data to sample for WordCounts.csv\n",
    "SampleFrac = 0.3\n",
    "#\n",
    "# Initialize paths for input files\n",
    "filepaths = glob.glob(input_dir_train + '*.txt')\n",
    "random.shuffle(filepaths)\n",
    "NSamples = float(len(filepaths))*SampleFrac\n",
    "NSamples = int(NSamples)\n",
    "filepaths = filepaths[0:NSamples]\n",
    "#\n",
    "num_files = len(filepaths)\n",
    "#\n",
    "# Delete pre-existing WordCounts.csv\n",
    "try:\n",
    "    os.remove(\"WordCounts.csv\")\n",
    "except OSError:\n",
    "    pass\n",
    "#\n",
    "#\n",
    "batch_size = 2000\n",
    "num_batches = ceil(float(num_files)/batch_size)\n",
    "num_batches = int(num_batches)\n",
    "# Begin reading input HTML files\n",
    "for i in range(num_batches):\n",
    "    p = multiprocessing.Pool()\n",
    "    if i != (num_batches-1):\n",
    "        filepaths_batch = filepaths[i*batch_size:(i+1)*batch_size]\n",
    "    else:\n",
    "        filepaths_batch = filepaths[i*batch_size:]\n",
    "    results = p.imap(get_word_counts, filepaths_batch)\n",
    "    while (True):\n",
    "        completed = results._index\n",
    "        progress = completed + i*batch_size\n",
    "        print(\"\\r--- Read {:,} out of {:,} HTML files\".format(progress, num_files), end='')\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(1)\n",
    "        if (completed == batch_size): \n",
    "            break\n",
    "        elif (progress == num_files):\n",
    "            break\n",
    "    p.close() \n",
    "    p.join()\n",
    "    WordCounts = pd.DataFrame(list(results))\n",
    "    if i==0:\n",
    "        WordCounts.to_csv(\"WordCounts.csv\", index=False, mode='a')\n",
    "    else:\n",
    "        WordCounts.to_csv(\"WordCounts.csv\", index=False, mode='a', header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
